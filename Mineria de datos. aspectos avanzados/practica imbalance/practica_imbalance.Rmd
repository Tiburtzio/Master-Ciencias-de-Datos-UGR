---
title: "imbalace"
author: "Alberto Armijo Ruiz"
date: "5 de febrero de 2019"
output: pdf_document
---

Cargamos librerías necesarias.
```{r}
library(caret)
library(dplyr)
library(pROC)
library(tidyr)
library(imbalance)
```


Cargamos funciones para entrenar los datos.
```{r}
learn_model <-function(dataset, ctrl,message){
  knn.fit <- train(Class ~ ., data = dataset, method = "knn", 
                   trControl = ctrl, preProcess = c("center","scale"), metric="ROC", 
                   tuneGrid = expand.grid(k = c(1,3,5,7,9,11)))
  knn.pred <- predict(knn.fit,newdata = dataset)
  #Get the confusion matrix to see accuracy value and other parameter values
  knn.cm <- confusionMatrix(knn.pred, dataset$Class,positive = "positive")
  knn.probs <- predict(knn.fit,newdata = dataset, type="prob")
  knn.roc <- roc(dataset$Class,knn.probs[,"positive"],color="green")
  return(knn.fit)
}

test_model <-function(dataset, knn.fit,message){
  knn.pred <- predict(knn.fit,newdata = dataset)
  #Get the confusion matrix to see accuracy value and other parameter values
  knn.cm <- confusionMatrix(knn.pred, dataset$Class,positive = "positive")
  print(knn.cm)
  knn.probs <- predict(knn.fit,newdata = dataset, type="prob")
  knn.roc <- roc(dataset$Class,knn.probs[,"positive"])
  #print(knn.roc)
  plot(knn.roc, type="S", print.thres= 0.5,main=c("ROC Test",message),col="blue")
  #print(paste0("AUC Test ",message,auc(knn.roc)))
  return(knn.cm)
}
```

Cargamos los conjuntos de datos que vamos a utilizar.
```{r}
dataset <- read.table("subclus.txt", sep=",")
dataset2 <- read.table("circle.txt", sep=",")
colnames(dataset) <- c("Att1", "Att2", "Class")
colnames(dataset2) <- c("Att1", "Att2", "Class")
```

Visualizamos los conjuntos de datos.
```{r}
library(ggplot2)
ggplot(dataset)+
  geom_point(aes(x=Att1,y=Att2,colour=Class))+
  ggtitle("Datos dataset subclus")


ggplot(dataset2)+
  geom_point(aes(x=Att1,y=Att2,colour=Class))+
  ggtitle("Datos dataset circle")

```
Como se puede ver, el primer dataset tiene pocos datos de  una clase que de otra, además, los datos de la clase minoritaria están mezclados con algunos de los casos de la clase mayoritaria. Para el segundo dataset, los datos de la clase minoritaria se encuentran en el centro, pero no se mezclan con los datos de la clase mayoritaria.

Calculamos su ratio de imbalance.
```{r}
imbalanceRatio(dataset)
imbalanceRatio(dataset2)
```

Como se puede ver, en ambos casos se puede ver que hay un gran desbalanceo entre las dos clases, siendo más exagerado para el dataset circle. Lo siguiente que vamos a hacer es crear particiones para cada uno de los dataset, y crear modelos y compararlos.
```{r}
set.seed(42)
dataset$Class <- relevel(dataset$Class,"positive")
index <- createDataPartition(dataset$Class, p = 0.7, list = FALSE)
train_data <- dataset[index, ]
test_data  <- dataset[-index, ]

dataset2$Class <- relevel(dataset2$Class,"positive")
index2 <- createDataPartition(dataset2$Class, p = 0.7, list = FALSE)
train_data2 <- dataset2[index2, ]
test_data2  <- dataset2[-index2, ]
```

Modelos con sin modificaciones a los datos.
```{r}
ctrl <- trainControl(method="repeatedcv",number=5,repeats = 3,
                     classProbs=TRUE,summaryFunction = twoClassSummary)
model.raw <- learn_model(train_data,ctrl,"RAW ")
#plot(model,main="Grid Search RAW")
#print(model.raw)
cm.original <- test_model(test_data,model.raw,"RAW ")

model.raw.2 = learn_model(train_data2,ctrl,"RAW")
cm.original.2 = test_model(test_data2,model.raw.2,"RAW")
```

Modelos con undersampling.
```{r}
ctrl <- trainControl(method="repeatedcv",number=5,repeats = 3,
                     classProbs=TRUE,summaryFunction = twoClassSummary,sampling = "down")

model.us <- learn_model(train_data,ctrl,"down ")
#plot(model,main="Grid Search RAW")
#print(model.raw)
cm.under <- test_model(test_data,model.us,"down")
model.us.2 <- learn_model(train_data2,ctrl,"down ")
cm.under.2 <- test_model(test_data2,model.us.2,"down")
```

Modelos con oversampling.
```{r}
ctrl <- trainControl(method="repeatedcv",number=5,repeats = 3,
                     classProbs=TRUE,summaryFunction = twoClassSummary,sampling = "up")

model.os <- learn_model(train_data,ctrl,"up ")
cm.over <- test_model(test_data,model.os,"up")
model.os.2 <- learn_model(train_data2,ctrl,"up ")
cm.over.2 <- test_model(test_data2,model.os.2,"up")
```

Modelos con SMOTE
```{r}
ctrl <- trainControl(method="repeatedcv",number=5,repeats = 3,
                     classProbs=TRUE,summaryFunction = twoClassSummary,sampling = "smote")

model.smt <- learn_model(train_data,ctrl,"smt")
cm.smote<- test_model(test_data,model.smt,"smt")

model.smt.2 <- learn_model(train_data2,ctrl,"smt")
cm.smote.2<- test_model(test_data2,model.smt.2,"smt")
```

Unimos los datos en una lista y comparamos las diferentes medidas entre ellos.
```{r}
models <- list(original = model.raw,
               under = model.us,
               over = model.os,
               smote = model.smt)

resampling <- resamples(models)
bwplot(resampling)

comparison <- data.frame(model = names(models),
                         Sensitivity = rep(NA, length(models)),
                         Specificity = rep(NA, length(models)),
                         Precision = rep(NA, length(models)),
                         Recall = rep(NA, length(models)),
                         F1 = rep(NA, length(models)))

for (name in names(models)) {
  cm_model <- get(paste0("cm.", name))
  
  comparison[comparison$model == name, ] <- filter(comparison, model == name) %>%
    mutate(Sensitivity = cm_model$byClass["Sensitivity"],
           Specificity = cm_model$byClass["Specificity"],
           Precision = cm_model$byClass["Precision"],
           Recall = cm_model$byClass["Recall"],
           F1 = cm_model$byClass["F1"])
}

comparison %>%
  gather(x, y, Sensitivity:F1) %>%
  ggplot(aes(x = x, y = y, color = model)) +
  geom_jitter(width = 0.2, alpha = 0.5, size = 3)+
  ggtitle("comparación métodos subclus")
```

```{r}
models <- list(original = model.raw.2,
               under = model.us.2,
               over = model.os.2,
               smote = model.smt.2)

resampling <- resamples(models)
bwplot(resampling)

comparison <- data.frame(model = names(models),
                         Sensitivity = rep(NA, length(models)),
                         Specificity = rep(NA, length(models)),
                         Precision = rep(NA, length(models)),
                         Recall = rep(NA, length(models)),
                         F1 = rep(NA, length(models)))

for (name in names(models)) {
  cm_model <- get(paste0("cm.", name,".2"))
  
  comparison[comparison$model == name, ] <- filter(comparison, model == name) %>%
    mutate(Sensitivity = cm_model$byClass["Sensitivity"],
           Specificity = cm_model$byClass["Specificity"],
           Precision = cm_model$byClass["Precision"],
           Recall = cm_model$byClass["Recall"],
           F1 = cm_model$byClass["F1"])
}

comparison %>%
  gather(x, y, Sensitivity:F1) %>%
  ggplot(aes(x = x, y = y, color = model)) +
  geom_jitter(width = 0.2, alpha = 0.5, size = 3)+
  ggtitle("comparación métodos circle")
```

Ahora, probaremos dos modelos nuevos de oversampling diferentes de SMOTE y compararemos los resultados.
```{r}
new_train = oversample(train_data,method="ADASYN",classAttr = "Class")
plotComparison(train_data,new_train,cols = 2,attrs = names(dataset)[1:2],classAttr = "Class")

new_train_blsmote = oversample(train_data,method="BLSMOTE",ratio=0.9,classAttr = "Class")
plotComparison(train_data,new_train_blsmote,cols = 2,attrs = names(dataset)[1:2],classAttr = "Class")
```

```{r}
new_train2 = oversample(train_data2,method="ADASYN",classAttr = "Class")
plotComparison(train_data2,new_train2,cols = 2,attrs = names(dataset2)[1:2],classAttr = "Class")

new_train_blsmote2 = oversample(train_data2,method="BLSMOTE",ratio=0.9,classAttr = "Class")
plotComparison(train_data2,new_train_blsmote2,cols = 2,attrs = names(dataset2)[1:2],classAttr = "Class")
```

Primero crearemos particiones para los dataset con oversampling, después creamos modelos para ambos datasets, y añadimos los resultados a las gráficas que hemos generado anteriormente.
```{r}
new_train$Class <- relevel(new_train$Class,"positive")
index <- createDataPartition(new_train$Class, p = 0.7, list = FALSE)
train_data <- new_train[index, ]
test_data  <- new_train[-index, ]

new_train2$Class <- relevel(new_train2$Class,"positive")
index2 <- createDataPartition(new_train2$Class, p = 0.7, list = FALSE)
train_data2 <- new_train2[index2, ]
test_data2  <- new_train2[-index2, ]
```

```{r}
ctrl <- trainControl(method="repeatedcv",number=5,repeats = 3,
                     classProbs=TRUE,summaryFunction = twoClassSummary)

model.adasyn <- learn_model(train_data,ctrl,"adasyn")
cm.adasyn<- test_model(test_data,model.adasyn,"adasyn")

model.adasyn.2 <- learn_model(train_data2,ctrl,"adasyn")
cm.adasyn.2<- test_model(test_data2,model.adasyn.2,"adasyn")
```

Repetimos el mismo proceso para los dataset generados con *BLSMOTE*.
```{r}
new_train_blsmote$Class <- relevel(new_train_blsmote$Class,"positive")
index <- createDataPartition(new_train_blsmote$Class, p = 0.7, list = FALSE)
train_data <- new_train_blsmote[index, ]
test_data  <- new_train_blsmote[-index, ]

new_train_blsmote2$Class <- relevel(new_train_blsmote2$Class,"positive")
index2 <- createDataPartition(new_train_blsmote2$Class, p = 0.7, list = FALSE)
train_data2 <- new_train_blsmote2[index2, ]
test_data2  <- new_train_blsmote2[-index2, ]
```

```{r}
ctrl <- trainControl(method="repeatedcv",number=5,repeats = 3,
                     classProbs=TRUE,summaryFunction = twoClassSummary)

model.blsmote <- learn_model(train_data,ctrl,"blsmote")
cm.blsmote <- test_model(test_data,model.blsmote,"blsmote")

model.blsmote.2 <- learn_model(train_data2,ctrl,"blsmote")
cm.blsmote.2<- test_model(test_data2,model.blsmote.2,"blsmote")
```

Ahora creamos de nuevo las gráficas comparativas de los modelos añadiendo los nuevos modelos generados.
```{r}
models <- list(original = model.raw,
               under = model.us,
               over = model.os,
               smote = model.smt,
               adasyn = model.adasyn,
               blsmote = model.blsmote)

resampling <- resamples(models)
bwplot(resampling)

comparison <- data.frame(model = names(models),
                         Sensitivity = rep(NA, length(models)),
                         Specificity = rep(NA, length(models)),
                         Precision = rep(NA, length(models)),
                         Recall = rep(NA, length(models)),
                         F1 = rep(NA, length(models)))

for (name in names(models)) {
  cm_model <- get(paste0("cm.", name))
  
  comparison[comparison$model == name, ] <- filter(comparison, model == name) %>%
    mutate(Sensitivity = cm_model$byClass["Sensitivity"],
           Specificity = cm_model$byClass["Specificity"],
           Precision = cm_model$byClass["Precision"],
           Recall = cm_model$byClass["Recall"],
           F1 = cm_model$byClass["F1"])
}

comparison %>%
  gather(x, y, Sensitivity:F1) %>%
  ggplot(aes(x = x, y = y, color = model)) +
  geom_jitter(width = 0.2, alpha = 0.5, size = 3)+
  ggtitle("comparación métodos subclus")
```

Como se puede ver en la gráfica, para el dataset *subclus* el modelo de adasyn y blsmote obtienen unos resultados muy parejos los dos. Esto es normal ya que las transformaciones que han realizado a los datos son muy parecidas.

```{r}
models <- list(original = model.raw.2,
               under = model.us.2,
               over = model.os.2,
               smote = model.smt.2,
               adasyn = model.adasyn.2,
               blsmote = model.blsmote.2)

resampling <- resamples(models)
bwplot(resampling)

comparison <- data.frame(model = names(models),
                         Sensitivity = rep(NA, length(models)),
                         Specificity = rep(NA, length(models)),
                         Precision = rep(NA, length(models)),
                         Recall = rep(NA, length(models)),
                         F1 = rep(NA, length(models)))

for (name in names(models)) {
  cm_model <- get(paste0("cm.", name,".2"))
  
  comparison[comparison$model == name, ] <- filter(comparison, model == name) %>%
    mutate(Sensitivity = cm_model$byClass["Sensitivity"],
           Specificity = cm_model$byClass["Specificity"],
           Precision = cm_model$byClass["Precision"],
           Recall = cm_model$byClass["Recall"],
           F1 = cm_model$byClass["F1"])
}

comparison %>%
  gather(x, y, Sensitivity:F1) %>%
  ggplot(aes(x = x, y = y, color = model)) +
  geom_jitter(width = 0.2, alpha = 0.5, size = 3)+
  ggtitle("comparación métodos circle")
```

Para el caso del dataset *circle* nos ocurre los mismo, los datos que obtenemos con ambos algoritmos son iguales; la razón es la misma que en el caso anterior, ambos algoritmos han realizado transformaciones muy parecidas a los datos (si se miran los scatter plots, se puede ver que ambos datos han reforzado la clase minoritaria en la frontera) y por ello los resultados son también muy parecidos.
